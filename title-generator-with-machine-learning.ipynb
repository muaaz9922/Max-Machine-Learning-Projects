{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8547993,"sourceType":"datasetVersion","datasetId":5107476}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport string\nimport numpy as np\nimport json\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\n#from keras.preprocessing import Tokenizer\nkeras.preprocessing.text.Tokenizer as Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku\n\ntf.random.set_seed(2)\nfrom numpy.random import seed\nseed(1)\n\n#loading the datasets\ndf1 = pd.read_csv('/kaggle/input/title-generator-datasets/USvideos.csv')\ndf2 = pd.read_csv('/kaggle/input/title-generator-datasets/CAvideos.csv')\ndf3 = pd.read_csv('/kaggle/input/title-generator-datasets/GBvideos.csv')\n\n#Now loading the datasets containing the category names\ndata1 = json.load(open('/kaggle/input/title-generator-datasets/US_category_id.json'))\ndata2 = json.load(open('/kaggle/input/title-generator-datasets/CA_category_id.json'))\ndata3 = json.load(open('/kaggle/input/title-generator-datasets/GB_category_id.json'))\n\n\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:07:33.938495Z","iopub.execute_input":"2024-05-29T09:07:33.939020Z","iopub.status.idle":"2024-05-29T09:07:39.035610Z","shell.execute_reply.started":"2024-05-29T09:07:33.938985Z","shell.execute_reply":"2024-05-29T09:07:39.033923Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      video_id  ...                                        description\n0  2kyS6SvSYSE  ...  SHANTELL'S CHANNEL - https://www.youtube.com/s...\n1  1ZAPwfrtAFY  ...  One year after the presidential election, John...\n2  5qpjK5DgCt4  ...  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...\n3  puqaWrEC7tY  ...  Today we find out if Link is a Nickelback amat...\n4  d380meD0W0M  ...  I know it's been a while since we did this sho...\n\n[5 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>trending_date</th>\n      <th>title</th>\n      <th>channel_title</th>\n      <th>category_id</th>\n      <th>publish_time</th>\n      <th>tags</th>\n      <th>views</th>\n      <th>likes</th>\n      <th>dislikes</th>\n      <th>comment_count</th>\n      <th>thumbnail_link</th>\n      <th>comments_disabled</th>\n      <th>ratings_disabled</th>\n      <th>video_error_or_removed</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2kyS6SvSYSE</td>\n      <td>17.14.11</td>\n      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n      <td>CaseyNeistat</td>\n      <td>22</td>\n      <td>2017-11-13T17:13:01.000Z</td>\n      <td>SHANtell martin</td>\n      <td>748374</td>\n      <td>57527</td>\n      <td>2966</td>\n      <td>15954</td>\n      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1ZAPwfrtAFY</td>\n      <td>17.14.11</td>\n      <td>The Trump Presidency: Last Week Tonight with J...</td>\n      <td>LastWeekTonight</td>\n      <td>24</td>\n      <td>2017-11-13T07:30:00.000Z</td>\n      <td>last week tonight trump presidency|\"last week ...</td>\n      <td>2418783</td>\n      <td>97185</td>\n      <td>6146</td>\n      <td>12703</td>\n      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>One year after the presidential election, John...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5qpjK5DgCt4</td>\n      <td>17.14.11</td>\n      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n      <td>Rudy Mancuso</td>\n      <td>23</td>\n      <td>2017-11-12T19:05:24.000Z</td>\n      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n      <td>3191434</td>\n      <td>146033</td>\n      <td>5339</td>\n      <td>8181</td>\n      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>puqaWrEC7tY</td>\n      <td>17.14.11</td>\n      <td>Nickelback Lyrics: Real or Fake?</td>\n      <td>Good Mythical Morning</td>\n      <td>24</td>\n      <td>2017-11-13T11:00:04.000Z</td>\n      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n      <td>343168</td>\n      <td>10172</td>\n      <td>666</td>\n      <td>2146</td>\n      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Today we find out if Link is a Nickelback amat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d380meD0W0M</td>\n      <td>17.14.11</td>\n      <td>I Dare You: GOING BALD!?</td>\n      <td>nigahiga</td>\n      <td>24</td>\n      <td>2017-11-12T18:01:41.000Z</td>\n      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n      <td>2095731</td>\n      <td>132235</td>\n      <td>1989</td>\n      <td>17518</td>\n      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>I know it's been a while since we did this sho...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we need to process our data so that we can use this data to train our machine learning model for the task of title generator. \n\n\nHere are all the data cleaning and processing steps that we need to follow:","metadata":{}},{"cell_type":"code","source":"def category_extractor(data):\n    i_d = [data['items'][i]['id'] for i in range(len(data['items']))]\n    title = [data['items'][i]['snippet']['title'] for i in range(len(data['items']))]\n    i_d = list(map(int,i_d))\n    category = zip(i_d,title)\n    category = dict(category)\n    return category\n\n\n#Create a new category column by mapping the category names to their id\ndf1['category_title'] = df1['category_id'].map(category_extractor(data1))\ndf2['category_title'] = df2['category_id'].map(category_extractor(data2))\ndf3['category_title'] = df3['category_id'].map(category_extractor(data3))\n\n#NOw join the dataframes\ndf = pd.concat([df1,df2,df3],ignore_index=True)\n\n#Now drop duplicates\ndf = df.drop_duplicates('video_id')\n\n#NOW collect the titles of entertainment videos\nentertainment = df[df['category_title'] == 'Entertainment']['title']\nentertainment = entertainment.tolist()\n\n#remove punctuations and convert text to lowercase\ndef clean_text(text):\n    text = ''.join(e for e in text if e not in string.punctuation).lower()\n    \n    text = text.encode('utf8').decode('ascii','ignore')\n    return text\n\ncorpus = [clean_text(e) for e in entertainment]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:22:48.040958Z","iopub.execute_input":"2024-05-29T09:22:48.041394Z","iopub.status.idle":"2024-05-29T09:22:48.247463Z","shell.execute_reply.started":"2024-05-29T09:22:48.041363Z","shell.execute_reply":"2024-05-29T09:22:48.246231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Generating Sequences\nNatural language processing tasks require input data in the form of a sequence of tokens. The first step after data cleansing is to generate a sequence of n-gram tokens.\n\nAn n-gram is an adjacent sequence of n elements of a given sample of text or vocal corpus. Elements can be words, syllables, phonemes, letters, or base pairs. In this case, the n-grams are a sequence of words in a corpus of titles. Tokenization is the process of extracting tokens from the corpus","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n\ntokenizer = Tokenizer()\ndef get_sequences_of_tokens(corpus):\n    #Get tokens\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    #convert to sequence of tokens\n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1,len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n    input_sequences.append(n_gram_sequence)\n    \n    return input_sequences, total_words\n\ninp_sequences, total_words=get_sequences_of_tokens(corpus)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:34:06.725145Z","iopub.execute_input":"2024-05-29T09:34:06.725576Z","iopub.status.idle":"2024-05-29T09:34:07.117810Z","shell.execute_reply.started":"2024-05-29T09:34:06.725545Z","shell.execute_reply":"2024-05-29T09:34:07.116024Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Padding the sequences\n\nSince the sequences can be of variable length, the sequence lengths must be equal. When using neural networks, we usually feed an input into the network while waiting for output. In practice, it is more efficient to process data in batches rather than one at a time.\n\n\nThis is done by using matrices [batch size x sequence length], where the length of the sequence corresponds to the longest sequence. \n\nIn this case, we fill the sequences with a token (usually 0) to fit the size of the matrix. This process of filling sequences with tokens is called filling. To enter the data into a training model, I need to create predictors and labels.\n\nI will create sequences of n-gram as predictors and the following word of n-gram as label:","metadata":{}},{"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len,padding='pre'))\n    predictors,label = input_sequences[:,:-1],input_sequences[:, -1]\n    label = ku.to_categorical(label,num_classes=total_words)\n    return predictors,label,max_sequence_len\n\npredictors,label,max_sequence_len = generate_padded_sequences(inp_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:43:03.885900Z","iopub.execute_input":"2024-05-29T09:43:03.886770Z","iopub.status.idle":"2024-05-29T09:43:03.896388Z","shell.execute_reply.started":"2024-05-29T09:43:03.886706Z","shell.execute_reply":"2024-05-29T09:43:03.894952Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Title Generator with LSTM Model\n\nThe LSTM model contains an additional state (the state of the cell) which essentially allows the network to learn what to store in \nthe long term state, what to delete and what to read.\n\nThe LSTM of this model contains three layers:\n\nInput layer: takes the sequence of words as input\n\nLSTM Layer: Calculates the output using LSTM units.\n\nDropout layer: a regularization layer to avoid overfitting\n\nOutput layer: calculates the probability of the next possible word on output\n\nNow I will use the LSTM Model to build a model for the task of Title Generator with Machine Learning:","metadata":{}},{"cell_type":"code","source":"def create_model(max_sequence_len,total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    #add input embedded layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    #add hidden layer 1 --LSTM layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    #ADd output layer\n    model.add(Dense(total_words,activation='softmax'))\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nmodel = create_model(max_sequence_len,total_words)\nmodel.fit(predictors,label,epochs=50,verbose=5)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:09:23.942161Z","iopub.execute_input":"2024-05-29T10:09:23.942705Z","iopub.status.idle":"2024-05-29T10:09:29.991028Z","shell.execute_reply.started":"2024-05-29T10:09:23.942668Z","shell.execute_reply":"2024-05-29T10:09:29.989515Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1/50\nEpoch 2/50\nEpoch 3/50\nEpoch 4/50\nEpoch 5/50\nEpoch 6/50\nEpoch 7/50\nEpoch 8/50\nEpoch 9/50\nEpoch 10/50\nEpoch 11/50\nEpoch 12/50\nEpoch 13/50\nEpoch 14/50\nEpoch 15/50\nEpoch 16/50\nEpoch 17/50\nEpoch 18/50\nEpoch 19/50\nEpoch 20/50\nEpoch 21/50\nEpoch 22/50\nEpoch 23/50\nEpoch 24/50\nEpoch 25/50\nEpoch 26/50\nEpoch 27/50\nEpoch 28/50\nEpoch 29/50\nEpoch 30/50\nEpoch 31/50\nEpoch 32/50\nEpoch 33/50\nEpoch 34/50\nEpoch 35/50\nEpoch 36/50\nEpoch 37/50\nEpoch 38/50\nEpoch 39/50\nEpoch 40/50\nEpoch 41/50\nEpoch 42/50\nEpoch 43/50\nEpoch 44/50\nEpoch 45/50\nEpoch 46/50\nEpoch 47/50\nEpoch 48/50\nEpoch 49/50\nEpoch 50/50\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x785340fbcb80>"},"metadata":{}}]},{"cell_type":"code","source":"# Assume X_test and y_test are your test data and labels\nloss, accuracy = model.evaluate(predictors, label)\n\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:23:44.578675Z","iopub.execute_input":"2024-05-29T10:23:44.580284Z","iopub.status.idle":"2024-05-29T10:23:44.994496Z","shell.execute_reply.started":"2024-05-29T10:23:44.580223Z","shell.execute_reply":"2024-05-29T10:23:44.993129Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 1.0000 - loss: 0.4205\nTest Loss: 0.4204827845096588\nTest Accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}