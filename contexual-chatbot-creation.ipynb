{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8912803,"sourceType":"datasetVersion","datasetId":5359450}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport json\nimport nltk\nnltk.download('punkt')\nfrom nltk.stem.lancaster import LancasterStemmer\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T09:29:23.921515Z","iopub.execute_input":"2024-07-09T09:29:23.921909Z","iopub.status.idle":"2024-07-09T09:29:24.093589Z","shell.execute_reply.started":"2024-07-09T09:29:23.921873Z","shell.execute_reply":"2024-07-09T09:29:24.092336Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"#IMporting the itents json file for the chatbot\nwith open('/kaggle/input/intents-for-chabot/intents.json') as json_data:\n    intents = json.load(json_data)\nintents","metadata":{"execution":{"iopub.status.busy":"2024-07-09T09:40:50.865897Z","iopub.execute_input":"2024-07-09T09:40:50.866784Z","iopub.status.idle":"2024-07-09T09:40:50.892270Z","shell.execute_reply.started":"2024-07-09T09:40:50.866746Z","shell.execute_reply":"2024-07-09T09:40:50.891097Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'intents': [{'tag': 'greeting',\n   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n   'responses': ['Hello, thanks for visiting',\n    'Good to see you again',\n    'Hi there, how can I help?'],\n   'context_set': ''},\n  {'tag': 'goodbye',\n   'patterns': ['Bye', 'See you later', 'Goodbye'],\n   'responses': ['See you later, thanks for visiting',\n    'Have a nice day',\n    'Bye! Come back again soon.']},\n  {'tag': 'thanks',\n   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n  {'tag': 'hours',\n   'patterns': ['What hours are you open?',\n    'What are your hours?',\n    'When are you open?'],\n   'responses': [\"We're open every day 9am-9pm\",\n    'Our hours are 9am-9pm every day']},\n  {'tag': 'location',\n   'patterns': ['What is your location?',\n    'Where are you located?',\n    'What is your address?',\n    'Where is your restaurant situated?'],\n   'responses': ['We are on the intersection of London Alley and Bridge Avenue.',\n    'We are situated at the intersection of London Alley and Bridge Avenue',\n    'Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK']},\n  {'tag': 'payments',\n   'patterns': ['Do you take credit cards?',\n    'Do you accept Mastercard?',\n    'Are you cash only?'],\n   'responses': ['We accept VISA, Mastercard and AMEX',\n    'We accept most major credit cards']},\n  {'tag': 'todaysmenu',\n   'patterns': ['What is your menu for today?',\n    'What are you serving today?',\n    \"What is today's special?\"],\n   'responses': [\"Today's special is Chicken Tikka\",\n    'Our speciality for today is Chicken Tikka']},\n  {'tag': 'deliveryoption',\n   'patterns': ['Do you provide home delivery?',\n    'Do you deliver the food?',\n    'What are the home delivery options?'],\n   'responses': ['Yes, we provide home delivery through UBER Eats and Zomato?',\n    'We have home delivery options through UBER Eats and Zomato'],\n   'context_set': 'food'},\n  {'tag': 'menu',\n   'patterns': ['What is your Menu?',\n    'What are the main course options?',\n    'Can you tell me the most delicious dish from the menu?',\n    \"What is the today's special?\"],\n   'responses': ['You can visit www.mymenu.com for menu options',\n    'You can check out the food menu at www.mymenu.com',\n    'You can check various delicacies given in the food menu at www.mymenu.com'],\n   'context_filter': 'food'}]}"},"metadata":{}}]},{"cell_type":"code","source":"stemmer = LancasterStemmer()\nwords = []\nclasses=[]\ndocuments = []\nignore = ['?']\n#NOW loop through each sentence in the intents patterns\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        #tokenize each and every word in the sentence\n        w = nltk.word_tokenize(pattern)\n        #add word to the word list\n        words.extend(w)\n        #add words to the documents\n        documents.append((w, intent['tag']))\n        #add tags to out classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n            \n#PErform stemming and lower each word as well as remove the duplicates\nwords = [stemmer.stem(w.lower()) for w in words if w not in ignore]\nwords = sorted(list(set(words)))\n\n#NOW remove the duplicates\nclasses = sorted(list(set(classes)))\n\nprint(len(documents),'documents')\nprint(len(classes),'classes',classes)\nprint(len(words),'unique stemmed words',words)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T09:52:52.751370Z","iopub.execute_input":"2024-07-09T09:52:52.751909Z","iopub.status.idle":"2024-07-09T09:52:52.777859Z","shell.execute_reply.started":"2024-07-09T09:52:52.751866Z","shell.execute_reply":"2024-07-09T09:52:52.776235Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"31 documents\n9 classes ['deliveryoption', 'goodbye', 'greeting', 'hours', 'location', 'menu', 'payments', 'thanks', 'todaysmenu']\n57 unique stemmed words [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n","output_type":"stream"}]},{"cell_type":"code","source":"#Now we create the training data\ntraining = []\noutput = []\n\n#creating an empty array for output\noutput_empty = [0] * len(classes)\n\n#Creating training set of bag of words for each sentence\nfor doc in documents:\n    #Initialize bag of words\n    bag = []\n    #list of tokenized words for the pattern\n    pattern_words = doc[0]\n    #Stemming each word\n    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n    #create bag of words array\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n        \n    #output is '1' for current tag and '0' for rest of other tags\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    \n    training.append([bag, output_row])\n    \n#Shuffling features and turning it into np.array\nrandom.shuffle(training)\ntraining = (training)\n\n#creating training lists\ntrain_x = list(training[:,0])\ntrian_y = list(training[:,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#NOw reseting underlying graph data\ntf.reset_default_graph()\n\n#building nueral network\nnet = tflearn.input_data(shape=[None,len(train_x[0])])\nnet = tflearn.fully_connected(net,10)\nnet = tflearn.fully_connected(net,10)\nnet = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\nnet = tflearn.regression(net)\n\n#Defining model and setting up tensorboard\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n\n#starting the training now\nmodel.fit(train_x,train_y,n_epochs=1000,batch_size=8,show_metric=True)\nmodel.save('model.tflearn')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\npickle.dump({'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open('training_data','wb'))\n\n#restoring all the data structures\ndata = pickle.load(open('training_data','rb'))\nwords = data['words']\nclasses = data['classes']\ntrain_x = data['train_x']\ntrain_y = data['train_y']\n\n\nwith open('intents.json') as json_data:\n    intents = json.load(json_data)\n    \n#now load the saved data\nmodel.load('.model.tflearn')\n\ndef clean_up_sentence(sentence):\n    #tokenize the pattern\n    setence_words = nltk.word_tokenize(sentence)\n    #stemming each word\n    sentence_words =[stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words\n\n#now returning the bag of words array :0 or 1 for each word in the bag that exists\ndef bow(sentence,words,show_details=False):\n    #tokenizing the pattern\n    sentence_words = clean_up_sentence(sentence)\n    #generating the bag of words\n    bag = [0]*len(words)\n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s:\n                bag[i] = 1\n                if show_details:\n                    print('found in bag: %s' % w)\n    return(np.array(bag))\n\n\n\nERROR_THRESHOLD = 0.30\ndef classify(sentence):\n    #generate probabilities from the model\n    results = model.predict([bow(sentence, words)])[0]\n    #filter out the predictions below the threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    #sort by the strength of porbability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]],r[1]))\n    #return tuple of intent and probability\n    return return_list\n\n\ndef response(sentence, userID='123', show_details=False):\n    results = classify(sentence)\n    #if we have a classification then find the matching intent tag\n    if results:\n        #loop as long as there are matchces to the process\n        while results:\n            for i in intents['intents']:\n                #find a tag matching the first result\n                if i['tag']== results[0][0]:\n                    #a random response from the intent\n                    return print(random.choice(i['response']))\n            results.pop(0)\n            ","metadata":{},"execution_count":null,"outputs":[]}]}