{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2277386,"sourceType":"datasetVersion","datasetId":1371688},{"sourceId":8955526,"sourceType":"datasetVersion","datasetId":5389620}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport warnings\nimport pickle\nwarnings.filterwarnings('ignore')\nfrom os import listdir\nfrom pickle import dump\nimport tensorflow\nimport keras\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.utils import load_img\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model\nfrom keras.layers import Input,Dense,LSTM,Embedding,Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T03:51:26.416058Z","iopub.execute_input":"2024-07-15T03:51:26.416899Z","iopub.status.idle":"2024-07-15T03:51:26.423614Z","shell.execute_reply.started":"2024-07-15T03:51:26.416856Z","shell.execute_reply":"2024-07-15T03:51:26.422271Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\n# Now extract the features from each photo in the directory\ndef extract_features(directory):\n    # Loading the pre-trained VGG16 model\n    model = VGG16(weights='imagenet', include_top=False)\n    model.layers.pop()  # restructuring the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    print(model.summary())  # summarizing the model\n\n    features = dict()  # Extracting features from the input photo\n    for name in listdir(directory):\n        filename = directory + '/' + name\n        image = load_img(filename, target_size=(224, 224))\n        image = img_to_array(image)  # converting the image pixels to a numpy array\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        image = preprocess_input(image)  # preparing the image for the VGG model\n        feature = model.predict(image, verbose=0)  # getting the features\n        image_id = name.split('.')[0]  # getting the image id\n        features[image_id] = feature  # storing the feature\n        print('>%s' % name)\n    return features\n\n\n\n#Now extracting the features from all the images\ndirectory = '/kaggle/input/flicker-8k-image-dataset-captionstxt/Images'\nfeatures = extract_features(directory)\nprint('Extracted Features --> %d' % len(features))\n#saving the file now\ndump(features, open('Image_Caption_Data/featues.pkl','wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\n#Creating a function that will load doc into memory\ndef load_doc(filename):\n    file = open(filename, 'r') #open the file as read only\n    text = file.read() # real all the text\n    file.close() #close the file\n    return text\n\n#NOw create the descriptions for images\ndef load_descriptions(doc):\n    mapping = dict()\n    #Now process the line\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        #Now take the first token as the image id and rest as the description\n        image_id, image_desc = tokens[0], tokens[1:]\n        #removeing the filenames from image id\n        image_id = image_id.split('.')[0]\n        #Now convert the description tokens back to string format\n        image_desc = ' '.join(image_desc)\n        #Now creating a list\n        if image_id not in mapping:\n            mapping[image_id] = list()\n        #NOW storing the description\n        mapping[image_id].append(image_desc)\n    return mapping\n\n\n\ndef clean_descriptions(descriptions):\n    #prepare translation table for removing punctuations\n    table = str.maketrans('','',string.punctuation)\n    for key,desc_list in descriptions.items():\n        desc = desc_list[i]\n        #tokeinizing\n        desc = desc.split()\n        #convert it to lower case\n        desc = [word.lower() for word in desc]\n        #removing punctuation from each token\n        desc = [w.translate(table) for w in desc]\n        #now remove hanging 's' and 'a'\n        desc = [word for word in desc if len(word)>1]\n        #removing tokens with numbers in them\n        desc = [word for word in desc if word.isalpha()]\n        #store as string\n        desc_list[i] = ' '.join(desc)\n        \n#Now convert the laoded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n    #Now build a list of all description strings\n    all_desc = set()\n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    return all_desc\n\n\n#NOw save descriptions to file one per line\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename,'w')\n    file.write(data)\n    file.close()\n    \nfilename = '/kaggle/input/flicker-8k-image-dataset-captionstxt/captions.txt'\n#Now load the descriptions \ndoc = load_doc(filename)\n#parse the descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ', % len(descriptions))\n#clean descriptions\nclean_descriptions(descriptions)\n#summarize vocabulary\nprint('Vocabulary Size: %d' % len(vocabulary))\n\n#save to file\nsave descriptions(descriptions, 'Image Caption Project Model Data/descriptions.txt')\n\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pickle import load\n#Now load the doc into the memory\ndef load_doc(filename):\n    #open the file as read only\n    file = open(filenme,'r')\n    #read all text\n    text = file.read()\n    #Now close the file\n    file.close()\n    return text\n\n\n#now load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    #process line by line\n    for line in doc.split('\\n'):\n        #skip empty lines\n        if len(line) < 1:\n            continue\n        #get the image indentifier\n        indentifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n#load clean descriptions into the memory\ndef load_clean_descriptions(filename, dataset):\n    #loading the document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        #split line by the white space\n        tokens = line.split()\n        #split id from descriptions\n        image_id , image_desc = token[0], tokens[1:]\n        #Now skip the images not in set \n        if image_id in dataset:\n            #create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            #wrap description in tokens\n            desc = 'startseq' + ' '.join(image_desc) + 'endseq'\n            #store\n            descriptions[image_id].append(desc)\n    return\n\n\n\n#Now load the photo features\ndef load_photo_features(filename, dataset):\n    #load all the features\n    all_features = load(open(filename, 'rb'))\n    #filter features\n    features = {k: all_features[k] for k in dataset}\n    return features\n\n\n#load the training dataset(6k)\nfilename = '/kaggle/input/flicker-8k-image-dataset-captionstxt/captions.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n#descriptions\ntrian_descriptions = load_clean_descriptions('/kaggle/input/flicker-8k-image-dataset-captionstxt/captions.txt')\nprint('Descriptions: train=%d' % len(train_descriptions))\n\n#photo features\ntrain_feautures = load_photo_features('Image_caption_Model.pkl', train)\nprint('Photos: train=%d' % len(train_features))\ntrain_descriptions\n\n\n\n#NOw convert a dictionary of clean descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n#Fit a tokenzier given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenzier()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n#get vocabulary size\ntokenzier = create_tokenizer(trian_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size: %d' % vocab_size)\n\n#calculate the length of the description with the most words\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\n#Now create sequences of images , input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list,photo):\n    x1, x2, y = list(), list(),list()\n    #walk through each description for the image\n    for desc in desc_list:\n        #encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        #split one sequence into multiple x,y paris\n        for i in range(1, len(seq)):\n            #split into input and output pair\n            in_seq, out_seq = seq[:i], seq[i]\n            #pad input sequence\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            #encode output sequence\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            #store\n            x1.append(photo)\n            x2.append(in_seq)\n            y.append(out_seq)\n    return array(x1), array(x2), array(y)\n\n\n#define the captioning model\ndef define_model(vocab_size,max_length):\n    #feature extractor model\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    #sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size,256,mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    \n    #decoder model\n    decorder1 = add([fe2,fe3])\n    decoder2 = Dense(256,activation='relu')(decoder1)\n    outputs = Dense(vocab_size,activation='softmax')(decoder2)\n    \n    #Tie it together\n    model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    #summarize the model\n    print(model.summary())\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we check the BLEU Scores for the Model","metadata":{}},{"cell_type":"code","source":"#Sentence BLEU Score calculation using NLTK\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Three','people','running','on','the','road'],['running','on','the','road']]\ncandidate = ['Person','is','running','on','road']\nscore = sentence_bleu(reference,candidate)\nprint(score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cumulative BLEU score calculation using NLTK\nfrom nltk.bleu_score import sentence_bleu\nreference = [['Three','people','running','on','the','road']]\ncadidate = ['Person','is','running','on','raod']\nprint('Cumulative 1-gram: %f' % sentence_bleu(reference,cadidate, weights=(1,0,0,0)))\nprint('Cumulative 2-gram: %f' % sentence_bleu(reference,cadidate, weights=(0.5,0.5,0,0)))\nprint('Cumulative 3-gram: %f' % sentence_bleu(reference,cadidate, weights=(0.33,0.33,0.33,0)))\nprint('Cumulative 4-gram: %f' % sentence_bleu(reference,cadidate, weights=(0.25,0.25,0.25,0.25)))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#map an integer to a worod\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n#generate a description for an image\ndef generate_desc(model,tokenizer,photo,max_length):\n    #seed the generation process\n    in_text = 'startseq'\n    #iterate over the whole length of the sequence\n    for i in range(max_length):\n        #integer encoding the input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        #pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        #predict the next word\n        yhat = model.predict([photo,sequence],verbose=0)\n        #nwo convert the probability to integer\n        yhat = argmax(yhat)\n        #map integer to word\n        word = word_for_id(yhat,tokenizer)\n        #Stop if we cannot map the word\n        if word in None:\n            break\n        #Now append as input for generating the next word\n        in_text += ' ' + word\n        #stop if we predict the end of the sequence\n        if word == 'enfseq':\n            break\n    return in_text\n\n\n\n\n#Now evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    #step over the whole set\n    for key, desc_list in descriptions.items():\n        #generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        #store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n        #calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0,0,0,0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5,0.5,0,0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3,0.3,0.3,0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25,0.25,0.25,0.25)))\n    \n    \n#Now prepare the training set\n#load the data\nfilename = '/kaggle/input/flicker-8k-image-dataset-captionstxt/captions.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n\n#descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt',train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n\n#prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary SIze: %d' % vocab_size)\n#determine the max sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n#prepare the testing set\n#loading\nfilename = '/kaggle/input/flicker-8k-image-dataset-captionstxt/captions.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n#descriptions\ntest_descriptions = load_cean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n#photo features\ntest_features = load_photo_features('features.pkl',test)\nprint('Photos: test=%d' % len(test_features))\n\n#load the model which has minimum loss in this case it was model_18\nfilename = 'model_18.h5'\nmodel = load_model(filename)\n#evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#REmove startseq and endseq\nquery = description\nstopwords = ['startseq','endseq']\nquerywords = query.split()\n\nresultwords = [word for word in querywords if word.lower() not in stopwords]\nresult = ' '.join(resultwords)\n\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now you can upload any image and check the outcome","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}